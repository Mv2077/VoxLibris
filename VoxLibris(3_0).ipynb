{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mv2077/VoxLibris/blob/main/VoxLibris(3_0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =========================================================\n",
        "#     C O G N I V E R S E  -  M L3\n",
        "# PROJECT: Robô Leitor com GRPO + Unsupervised + RF + 2 Agentes\n",
        "# ========================================================="
      ],
      "metadata": {
        "id": "e1QNgkUa4Nia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install numpy --upgrade --force-reinstall\n",
        "!pip install --no-cache-dir --force-reinstall pyarrow==14.0.2\n",
        "!pip install unsloth\n",
        "!pip install -U sentence-transformers scikit-learn pypdf2 PyCryptodome accelerate einops\n",
        "!pip install vllm\n",
        "!pip install --upgrade transformers bitsandbytes\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "dbopOkpY-slG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Imports\n",
        "%%capture\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import PyPDF2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, silhouette_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import os\n",
        "import wandb\n",
        "from sklearn.metrics import silhouette_score, classification_report, confusion_matrix, f1_score"
      ],
      "metadata": {
        "id": "M-7od3dcvoO_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Requests\n",
        "\n",
        "# --- Download do PDF ---\n",
        "PDF_URL = 'https://cdn.shopify.com/s/files/1/2081/8163/files/022-I-FOUND-A-FROG-Free-Childrens-Book-By-Monkey-Pen.pdf?v=1589890638'\n",
        "PDF_FILENAME = 'Main.pdf'\n",
        "\n",
        "print(f\"A descarregar PDF de: {PDF_URL}...\")\n",
        "r = requests.get(PDF_URL)\n",
        "with open(PDF_FILENAME, 'wb') as f:\n",
        "    f.write(r.content)\n",
        "print(\"Download concluído!\")\n",
        "\n",
        "def extract_pages(pdf_path):\n",
        "    pages = []\n",
        "    if not os.path.exists(pdf_path): return pages\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text and len(text.strip()) > 10:\n",
        "                pages.append(text.strip())\n",
        "    return pages\n",
        "\n",
        "pages = extract_pages(PDF_FILENAME)\n",
        "print(f\"Total de páginas extraídas: {len(pages)}\")\n"
      ],
      "metadata": {
        "id": "VWrbp6Xg0g87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Business and Data Understanding\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**O nosso negócio seria desenvolver um robô que fizesse leitura de livros.**\n",
        "Ele necessitaria de ter uma base de dados onde nós introduzissemos livros e depois era só pedir uma leitura de x livro e ele lia.\n"
      ],
      "metadata": {
        "id": "inL1NJ5yv9KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data engeneering\n",
        "\n",
        "# ============================================================\n",
        "# 1 — DATA ENGINEERING\n",
        "# Extração → Embeddings → Clustering → RF → Dataset\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "PDF_FILENAME = \"Main.pdf\" # Changed from \"livro.pdf\" to \"Main.pdf\"\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Extração do PDF\n",
        "# -------------------------------\n",
        "def extract_pages(pdf_path):\n",
        "    pages = []\n",
        "    if not os.path.exists(pdf_path): return pages\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text and len(text.strip()) > 10:\n",
        "                pages.append(text.strip())\n",
        "    return pages\n",
        "\n",
        "pages = extract_pages(PDF_FILENAME)\n",
        "print(f\"Total de páginas extraídas: {len(pages)}\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Embeddings\n",
        "# -------------------------------\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedder.encode(pages, show_progress_bar=True)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Clustering (descobrir tópicos)\n",
        "# -------------------------------\n",
        "kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
        "cluster_ids = kmeans.fit_predict(embeddings)\n",
        "print(\"Clustering concluído!\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Avaliação do Clustering\n",
        "# -------------------------------\n",
        "sil_score = silhouette_score(embeddings, cluster_ids)\n",
        "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Random Forest como “Juiz”\n",
        "# -------------------------------\n",
        "vectorizer = TfidfVectorizer(max_features=1500)\n",
        "X = vectorizer.fit_transform(pages)\n",
        "y = cluster_ids\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n=== Avaliação do RF (Juiz) ===\")\n",
        "print(classification_report(y_test, y_pred := rf.predict(X_test)))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Treinar modelo final\n",
        "rf_final = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_final.fit(X, y)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Criar dataset final\n",
        "# -------------------------------\n",
        "dataset = []\n",
        "for i, text in enumerate(pages):\n",
        "    dataset.append({\n",
        "        \"id\": f\"page_{i}\",\n",
        "        \"text\": text,\n",
        "        \"cluster_id\": int(cluster_ids[i]),\n",
        "        \"meta\": {\"page\": i, \"book\": \"Livro_Exemplo\"}\n",
        "    })\n",
        "\n",
        "with open(\"dataset_prepared.json\", \"w\", encoding=\"utf8\") as f:\n",
        "    json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Dataset criado: dataset_prepared.json\")"
      ],
      "metadata": {
        "id": "5aDakNpWw_n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Wandb Starter\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "# Define config before wandb.init\n",
        "config = GRPOConfig(\n",
        "    learning_rate=1e-5,\n",
        "    num_generations=4,\n",
        "    max_completion_length=200,\n",
        "    per_device_train_batch_size=1\n",
        ")\n",
        "\n",
        "wandb.init(\n",
        "    project=\"grpo-lora-reader\",\n",
        "    name=\"treino_lora_grpo\",\n",
        "    config={\n",
        "        \"model\": \"Llama-3.2-3B-Instruct\",\n",
        "        \"batch_size\": config.per_device_train_batch_size,\n",
        "        \"lr\": config.learning_rate,\n",
        "        \"num_generations\": config.num_generations,\n",
        "        \"max_completion_length\": config.max_completion_length\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "nqwWT2_-ybI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n_MDLKOu4iW"
      },
      "outputs": [],
      "source": [
        "#@title Model Engineering\n",
        "# ============================================================\n",
        "# ReaderAgent (Llama + LoRA + GRPO)\n",
        "# Summarizer\n",
        "# Q&A\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2 — MODELING\n",
        "# ReaderAgent (LLaMA) + AnalystAgent + Reward Function\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from unsloth import FastLanguageModel\n",
        "from sentence_transformers import util\n",
        "\n",
        "# -------------------------------\n",
        "# Perplexidade\n",
        "# -------------------------------\n",
        "tok_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\").eval()\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = tok_gpt2(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        loss = model_gpt2(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "    return float(torch.exp(loss).item())\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# AnalystAgent (Reward)\n",
        "# -------------------------------\n",
        "class AnalystAgent:\n",
        "    def __init__(self, vectorizer, rf, embedder, log_to_wandb=True):\n",
        "        self.vec = vectorizer\n",
        "        self.rf = rf\n",
        "        self.embedder = embedder\n",
        "        self.history = []\n",
        "        self.log_to_wandb = log_to_wandb\n",
        "\n",
        "    def evaluate(self, original, generated):\n",
        "        # Similaridade\n",
        "        e1 = self.embedder.encode(original, convert_to_tensor=True)\n",
        "        e2 = self.embedder.encode(generated, convert_to_tensor=True)\n",
        "        sim = float(util.pytorch_cos_sim(e1, e2).item())\n",
        "\n",
        "        # Perplexity Score\n",
        "        ppl = calculate_perplexity(generated)\n",
        "        ppl_score = 1 / (1 + 0.1 * np.log(ppl + 1))\n",
        "\n",
        "        # Tópico (RF)\n",
        "        orig_topic = self.rf.predict(self.vec.transform([original]))[0]\n",
        "        gen_topic = self.rf.predict(self.vec.transform([generated]))[0]\n",
        "        topic_score = 1.0 if orig_topic == gen_topic else 0.0\n",
        "\n",
        "        # Reward final\n",
        "        reward = 0.4 * sim + 0.2 * ppl_score + 0.4 * topic_score\n",
        "        reward = float(max(0, min(1, reward)))  # clamp\n",
        "\n",
        "        self.history.append({\n",
        "            \"similarity\": sim,\n",
        "            \"perplexity\": ppl_score,\n",
        "            \"topic_match\": topic_score,\n",
        "            \"reward\": reward\n",
        "        })\n",
        "\n",
        "        return reward\n",
        "\n",
        "\n",
        "analyst = AnalystAgent(vectorizer, rf_final, embedder)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Prompts + map\n",
        "# -------------------------------\n",
        "def build_prompt(ex):\n",
        "    system = (\n",
        "        \"Tu és um leitor automático.\\n\"\n",
        "        \"Gera uma leitura natural do texto.\\n\"\n",
        "    )\n",
        "    user = f\"CLUSTER={ex['cluster_id']}\\nTexto:\\n{ex['text']}\\n\"\n",
        "\n",
        "    prompt = system + \"\\n\" + user\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"original_text\": ex[\"text\"],\n",
        "        \"cluster_id\": ex[\"cluster_id\"]\n",
        "    }\n",
        "\n",
        "training_data = [build_prompt(ex) for ex in dataset]\n",
        "\n",
        "\n",
        "# Mapa prompt → original_text\n",
        "prompt_str_map = {item[\"prompt\"]: item[\"original_text\"] for item in training_data}\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Função de Reward GRPO\n",
        "# -------------------------------\n",
        "def grpo_reward(prompts, completions, **kwargs):\n",
        "    rewards = []\n",
        "    for p, c in zip(prompts, completions):\n",
        "        prompt_text = p if isinstance(p, str) else str(p)\n",
        "        original = prompt_str_map.get(prompt_text, \"\")\n",
        "        generated = str(c)\n",
        "        rewards.append(analyst.evaluate(original, generated))\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data[0][\"prompt\"][:500])"
      ],
      "metadata": {
        "id": "ZDAVVOvi_P3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training\n",
        "# ============================================================\n",
        "# 3 — TRAINING (GRPO)\n",
        "# ============================================================\n",
        "\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(model, r=8)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    reward_funcs=[grpo_reward],\n",
        "    args=config,\n",
        "    train_dataset=training_data\n",
        ")\n",
        "\n",
        "print(\"Iniciando treino...\")\n",
        "trainer.train()\n",
        "print(\"Treino concluído!\")\n",
        "\n",
        "\n",
        "model.save_pretrained(\"modelo_lora_grpo\")\n",
        "tokenizer.save_pretrained(\"modelo_lora_grpo\")"
      ],
      "metadata": {
        "id": "3veAy7FY7hZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quality Assurance\n",
        "\n",
        "# ============================================================\n",
        "# 4 — QUALITY\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n=== RELATÓRIO DE QUALIDADE ===\\n\")\n",
        "\n",
        "if len(analyst.history) == 0:\n",
        "    print(\"❗ Histórico vazio — reward function não foi chamada.\")\n",
        "else:\n",
        "    df = pd.DataFrame(analyst.history)\n",
        "\n",
        "    print(df.describe())\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df[\"reward\"], label=\"reward\")\n",
        "    plt.plot(df[\"similarity\"], label=\"similarity\")\n",
        "    plt.plot(df[\"perplexity\"], label=\"perplexity\")\n",
        "    plt.plot(df[\"topic_match\"], label=\"topic_match\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Evolução das Métricas\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nÚltimas 10 avaliações:\")\n",
        "    print(df.tail(10))\n"
      ],
      "metadata": {
        "id": "7_7gLlfv6V0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DEPLOYMENT\n",
        "\n",
        "print(\"A guardar o modelo aprovado para produção...\")\n",
        "\n",
        "# Guardar o modelo final pós-GRPO\n",
        "model.save_pretrained(\"modelo_final_aprovado\")\n",
        "tokenizer.save_pretrained(\"modelo_final_aprovado\")\n",
        "\n",
        "print(\"Modelo salvo em '/content/modelo_final_aprovado'\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Carregar o modelo final (como no inference real)\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nA carregar o modelo para inferência...\")\n",
        "\n",
        "modelo_prod, tokenizer_prod = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"modelo_final_aprovado\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ativar LoRA (obrigatório para inferência)\n",
        "modelo_prod = FastLanguageModel.for_inference(modelo_prod)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Função de Inferência (API final do projeto)\n",
        "# ------------------------------------------------------------\n",
        "def assistente_leitura(texto_usuario):\n",
        "    # Formato do prompt igual ao do treino\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Tu és um assistente útil que resume e explica textos.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Texto: {texto_usuario}\"}\n",
        "    ]\n",
        "\n",
        "    # Template de chat (Unsloth)\n",
        "    input_ids = tokenizer_prod.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(modelo_prod.device)\n",
        "\n",
        "    # Geração\n",
        "    with torch.no_grad():\n",
        "        output = modelo_prod.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=250,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    # Decodificação correta\n",
        "    generated_text = tokenizer_prod.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remover o prompt do início\n",
        "    clean_text = generated_text.replace(messages[0][\"content\"], \"\")\n",
        "    return clean_text.strip()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Demonstração real do sistema\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n--- SIMULAÇÃO DE DEPLOYMENT ---\")\n",
        "\n",
        "texto_teste = (\n",
        "    \"The little frog looked at the sky and wondered why it was so blue. \"\n",
        "    \"He jumped happily into the pond.\"\n",
        ")\n",
        "\n",
        "print(f\"Input do utilizador:\\n{texto_teste}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "resultado = assistente_leitura(texto_teste)\n",
        "print(f\"Resposta do modelo:\\n{resultado}\")\n"
      ],
      "metadata": {
        "id": "ULFxbmfSyaG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Monitoring and Maintenance\n",
        "print(\"Monitorizar resultados e coletar feedback para manter e melhorar o desempenho do robô assistente.\")"
      ],
      "metadata": {
        "id": "woAgMepWykXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}